= Имплементација система

Имплементациони део рада разматра конкретну реализацију развијеног радног оквира и описује како су концепти представљени у архитектури претворени у функционалну целину. У овом поглављу приказана су кључна техничка решења која омогућавају учитавање конфигурације, извршавање провера активности сервиса, управљање стањем сервиса, спровођење корективних мера, као и генерисање метрика и алармних сигнала. Радни оквир је имплементиран у програмском језику Go, чији механизми конкуренције засновани на горутинима (енгл. _goroutines_) и модуларна структура пакета омогућавају ефикасно паралелно извршавање провера, јасну организацију кода и једноставну интеграцију са екстерним компонентама.

У наставку се разматра конкретна програмска реализација главних функција система. Приказана је структура пројекта, намена основних пакета, обрада конфигурационе датотеке и кључни делови програмске логике који управљају проверама и стањем сервиса. Додатно су описани механизми конкурентног извршавања, комуникација са _Docker_ окружењем и интеграција са системима за метрике и алармирање. На овај начин поглавље пружа увид у техничке аспекте радног оквира и објашњава како су идеје из претходног поглавља реализоване у пракси.

== Организација пројекта

Пројекат је организован у складу са _Go_ конвенцијама, где је код подељен на пакете према функционалној одговорности. У кореном директоријуму налазе се улазна тачка апликације и конфигурациона датотека, док је имплементација радног оквира смештена у директоријум _internal_, који садржи доменску логику надзора, интеграцију са _Docker_ окружењем, систем метрика и пратеће помоћне модуле. Оваква организација обезбеђује јасно раздвајање одговорности и спречава употребу интерних компоненти од стране спољашњих пакета. У табели  #link(<tbl:organizacija_projekta>)[3] приказани су основни пакети и њихова улога у систему.

#figure(
  table(
    columns: 2,
    align: (col, row) => (left, left).at(col),
    inset: 6pt,

    [Пакет / датотека], [Улога у систему],

    [_config/_],
    [Садржи _config.yml_ и модул _config.go_ задужен за учитавање, парсирање и валидирање конфигурације.],

    [_internal/communication_],
    [Kомуникацијa са сервисима који се прате (_HTTP_, _TCP_, _gRPC_). Јединствен интерфејс који логика за проверу активности користи без обзира на протокол.],

    [_internal/helpers_],
    [Пакет са помоћним функцијама које се користе у више модула система.],

    [_internal/metrics_],
    [Имплементација метрика (нпр. бројачи успешних, неуспешних провера...).],

    [_internal/monitoring_],
    [Садржи конфигурационе датотеке за _Prometheus_ и _Alertmanager_.],

    [_internal/operations_],
    [Имплементација операција над _Docker_ окружењем (нпр. поновно покретање сервиса).],

    [_internal/probes_],
    [Логика за rad са свим типовима проверама (покретање, спремност, активност). Повезује конфигурацију са конкретним типом проверe и начином извршавања.],

    [_internal/runner_],
    [Покретање горутина за периодичне провере и координација свих компоненти надзора.],

    [_main.go_],
    [Покреће главни циклус радног оквира. Учитава конфигурацију и иницијализује компоненте.],
  ),
  caption: [Организација пројекта]
)<tbl:organizacija_projekta>

== Конфигурација система

Конфигурација представља један од кључних елемената развијеног система, јер одређује које се сервиси посматрају, које се провере над њима извршавају и под којим условима се доносе одлуке о исправности. Уместо да се ови параметри уграђују у изворни код, они су издвојени у посебну конфигурациону датотеку у _YAML_ формату, што омогућава да се понашање система прилагоди различитим окружењима без потребе за изменама у имплементацији. Конфигурацију обично припрема особа која познаје карактеристике сервиса који се надзиру, јер управо од правилно постављених интервала, прагова и типова провера зависи стабилност и поузданост надзорног система.

=== Структура _YAML_ конфигурације

Конфигурациона датотека организована је као листа контејнера (енгл. _containers_), где за сваки контејнер постоји назив сервиса и дефиниције провера покретања, спремности и активности. Свака провера садржи опис метода комуникације (_HTTP_, _TCP_ или _gRPC_), адресу сервиса (путања, порт, _host_), и временске параметре као што су почетно кашњење (енгл. _initialDelaySeconds_), интервал између провера (енгл. _periodSeconds_), праг за број узастопних неуспеха (енгл. _failureThreshold_) и опциони timeout (енгл. _timeoutSeconds_). На листингу #link(<lst:yaml_config>)[1] приказан је пример исправне _YAML_ конфигурације за један сервис.

#figure(
```yaml
containers:
  - name: service-a
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
        host: service-a
      initialDelaySeconds: 0
      periodSeconds: 2
      failureThreshold: 30

    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        host: service-a
      initialDelaySeconds: 5
      periodSeconds: 10
      failureThreshold: 3

    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
        host: service-a
      initialDelaySeconds: 2
      periodSeconds: 5
      failureThreshold: 3

```,
    caption: [Пример исправне _YAML_ конфигурације.]
)<lst:yaml_config>

Овај пример илуструје случај HTTP провера за сервис који подржава три различита типа провера. Иста структура примењује се и за сервисе који користе _gRPC_ или _TCP_ провере, уз одговарајућу измену секција _grpc_: или _tcpSocket_:.

#pagebreak()

=== Мапирање конфигурације на _Go_ структуре

Структура _YAML_ датотеке директно је пресликана на модел података у програмском језику _Go_. Основни тип _Config_ садржи листу контејнера, док је сваки контејнер представљен структуром _Container_. Параметри провера описани су структуром _Probe_, која подржава три различита модела комуникације путем опционих поља. На листингу #link(<lst:go_config>)[2] приказане су _Go_ структуре које описују конфигурацију.


#figure(
```go
type Config struct {
	Containers []Container `yaml:"containers"`
}

type Container struct {
	Name           string `yaml:"name"`
	StartupProbe   *Probe `yaml:"startupProbe,omitempty"`
	LivenessProbe  *Probe `yaml:"livenessProbe,omitempty"`
	ReadinessProbe *Probe `yaml:"readinessProbe,omitempty"`
}

type Probe struct {
	HTTPGet             *HTTPGet   `yaml:"httpGet,omitempty"`
	TCPSocket           *TCPSocket `yaml:"tcpSocket,omitempty"`
	GRPC                *GRPC      `yaml:"grpc,omitempty"`
	InitialDelaySeconds int        `yaml:"initialDelaySeconds,omitempty"`
	PeriodSeconds       int        `yaml:"periodSeconds"`
	FailureThreshold    int        `yaml:"failureThreshold"`
	TimeoutSeconds      int        `yaml:"timeoutSeconds,omitempty"`
}

type HTTPGet struct {
	Path string `yaml:"path"`
	Port int    `yaml:"port"`
	Host string `yaml:"host"`
}

type TCPSocket struct {
	Port int    `yaml:"port"`
	Host string `yaml:"host"`
}

type GRPC struct {
	Port    int    `yaml:"port"`
	Host    string `yaml:"host,omitempty"`
	Service string `yaml:"service,omitempty"`
}
```,
    caption: [Пример исправне YAML конфигурације.]
)<lst:go_config>


=== Учитавање и валидирање конфигурације

При покретању система конфигурациона датотека се учитава коришћењем _YAML unmarshal_ механизма, који садржај датотеке преводи у одговарајуће _Go_ структуре. Након парсирања врши се додатна валидација, чији је циљ да се открију логичке грешке у конфигурацији пре него што надзор почне са радом.

Примери валидаторских правила укључују:
- листа _containers_ не сме бити празна
- сваки контејнер мора имати бар један дефинисан тип провере 
- вредности _periodSeconds_ и _failureThreshold_ морају бити веће од нуле
- комбинација параметара мора бити логички исправна (нпр. _TCP_ провера не сме садржати _path_)

У случају погрешне или непотпуне конфигурације систем одмах враћа грешку, спречавајући покретање система провере активности сервиса са неисправним параметрима.

== Имплементација механизма провере активности сервиса

Механизам активности сервиса представља језгро радног оквира. Он повезује конфигурацију дефинисану у YAML датотеци са конкретним _HTTP_/_TCP_/_gRPC_ проверама које се периодично извршавају над циљаним сервисима. Основна идеја је да се свака провера (покретање, спремност, активност) опише на декларативан начин, а да радни оквир преузме бригу о њеном извршавању.

=== Интерни модел провера

Након учитавања конфигурације, свака дефинисана провера се пресликава у интерни модел који садржи:

- тип провере (_ProbeType_: _liveness_, _readiness_, _startup_),
- протокол (_HTTP_, _TCP_ или _gRPC_),
- циљ (_target_: _hostname_, порт и, по потреби, путања),
- параметре извршавања:
  - период понављања провере (_periodSeconds_),
  - максимално трајање једног покушаја (_timeoutSeconds_),
  - број узастопних успеха и неуспеха након којих се мења стање (_successThreshold_, _failureThreshold_),
  - почетно кашњење пре прве провере (_initialDelaySeconds_).

На тај начин систем не ради директно над _YAML_ структуром, већ над јасно дефинисаним објектима који описују шта тачно треба проверавати за сваки сервис и сваки тип провере.

=== Покретање провера

За сваки описани тип провере систем покреће посебну позадинску горутину. Свака таква горутина представља једну петљу провере која:

- по потреби сачека иницијално кашњење,
- у регуларним интервалима (према _periodSeconds_) иницира појединачну проверу,
- прикупља исход провере (успех или неуспех, трајање извршавања, опис грешке),
- резултат шаље на заједнички канал који користе компоненте за управљање животним циклусом и за метрике.

Оваква организација омогућава да се сваки тип провере извршава независно од других, без блокирања главне нити програмa. Систем користи контексте (енгл. _context_) како би могао да заустави све активне провере приликом гашења или поновног покретања.

=== _HTTP_ провере

За HTTP провере конструише се _URL_ на основу _hostname_-а, порта и путање задате у конфигурацији. Провера се извршава као _HTTP GET_ захтев, уз ограничење максималног трајања (енгл. _timeout_).

Провера се сматра успешном ако је:

- успостављена мрежна конекција,
- стигао одговор у оквиру задатог временског ограничења,
- статусни код у опсегу 2xx или 3xx.

У случају да дође до прекорачења ограниченог максималног трајања или статусног кода у формату (4xx, 5xx), провера се бележи као неуспех. Поред самог исхода, мери се и трајање захтева, што се накнадно користи за метрике. На листингу #link(<lst:go_config>)[3] приказана је функција _HTTP_ провере.

#figure(
```go
func HTTPCheck(ctx context.Context, t Target) (bool, error) {
	timeout := helpers.GetTimeout(t.Timeout)
	client := &http.Client{Timeout: timeout}

	url := helpers.BuildHTTPURL(t.Host, t.Port, t.Path)
	req, _ := http.NewRequestWithContext(ctx, http.MethodGet, url, nil)

	resp, err := client.Do(req)
	if err != nil {
		return false, err
	}
	defer resp.Body.Close()

	return helpers.IsSuccessStatus(resp.StatusCode), nil
}
```,
    caption: [Функција _HTTP_ провере.]
)<lst:http>

=== _TCP_ провере

_TCP_ провере су намењене сервисима који немају _HTTP_ интерфејс, али је битно да је порт доступан. У том случају, провера се своди на покушај успостављања _TCP_ конекције ка задатом _host_-у и порту, у оквиру дефинисаног максималног временског трајања.

- Ако је конекција успешно успостављена и одмах затворена, провера се сматра успешном.
- Ако успостављање конекције истекне или јави грешку, провера се сматра неуспешном.

Оваква провера је лагана и не захтева разумевање протокола изнад _TCP_-а.

=== _gRPC_ провере

За _gRPC_ провере систем користи стандардизовани _gRPC health checking_ протокол (сервис _grpc.health.v1.Health_). Да би провера функционисала, циљни _gRPC_ сервис треба да имплементира овај сервис.

Приликом овог типа провере радни оквир ради следеће:

- успоставља _gRPC_ конекцију ка задатом _host_-у и порту, са задатим временским ограничењем,
- позива метод _Check_, уз назив _gRPC_ сервиса који се проверава,
- тумачи добијени статус:
  - ако је статус _SERVING_ онда провера је успешна,
  - сваки други статус или грешка приликом позива говори да се провера сматра неуспешном.

И у овом случају се бележе и исход и трајање, како би се накнадно ажурирале метрике.

=== Механизам за обраду резултата

Иако _HTTP_, _TCP_ и _gRPC_ провере користе различите протоколе, систем их на излазном нивоу третира на исти начин. Сваки појединачни покушај провере производи структуру резултата која садржи:

- идентификацију провере (тип, име, контејнер, протокол),
- логички исход (успех/неуспех),
- евентуалну грешку (нпр. _status code_ 500),
- трајање провере,
- временски печат.

#pagebreak()

Ови резултати се шаљу на заједнички канал који користе:

- компонента за управљање животним циклусом сервиса, која на основу резултата провере конекције и дозвољеног броја грешки мења стање сервиса и евентуално иницира поновно покретање сервиса;

- компонента за метрике, која резултате преводи у _Prometheus_ _counter_ метрике.

== Имплементација управљања животним циклусом сервиса

Након што сисетм прикупи резултате свих провера (_HTTP_/_TCP_/_gRPC_), потребно је донети одлуку о томе у ком се стању циљни сервис тренутно налази, као и како наш радни оквир да реагује. Управљање животним циклусом представља механизам који на основу бројача неуспешних провера, броја дозвољених неуспешних провера и типова провера одређује:

- да ли је сервис активан или није (_Healthy_ или _Unhealthy_),  
- да ли је сервис спреман или није (_Ready_ или _NotReady_),
- да ли је фаза покретања успешно завршена,
- да ли је потребно иницирати поновно покретање сервиса,
- када се мења унутрашње стање система.
    
Цео механизам може се посматрати као континуирана обрада резултата које шаљу позадинске горутине задужене за провере активности сервиса.

=== Модел стања сервиса

Сваки сервис који систем надгледа има три независне компоненте стања:

1. Стање активности (_liveness_)   
    Одређује да ли је сервис активан, односно да ли је у функционалном стању да настави са радом. Ако ова провера узастопно не успева, сервис се сматра _Unhealthy_ и систем може извршити поновно покретање сервиса.
    
2. Стање спремности (_readiness_)
    Одређује да ли је сервис спреман да прихвата саобраћај. Ова провера не подразумева да сервис није активан, већ само да можда тренутно није у стању да успешно обрађује захтеве.
    
3. Стање покретања (_startup_) 
    Представља посебну фазу рада сервиса, у којој се очекује да сервис стабилизује своје окружење пре него што се укључи логика за проверу активности сервиса. Уколико постоје, прво се изврше ове провере па тек након успешноси крећу провере активности и спремности сервиса.

=== Управљање стањем активности

Стање активности представља основну способност сервиса да функционише без проблема. Ово стање се утврђује на основу резултата провера активности које се извршавају у задатим интервалима. Сваки успешан исход провере указује на то да је сервис активан, док сваки неуспех повећава бројач неуспешних провера. Када број узастопних неуспеха достигне дефинисан праг толеранције, сервис се сматра неактивним, односно нефункционалним. На листингу #link(<lst:liveness>)[4] приказан је део кода који реализује ову логику.

#figure(
```go
ok, _ := service.Check(ctx, s.Target)
if ok {
    // логика када систем задржава активно стање
    return
}

*fail++
if !*bad && *fail >= s.FailureThreshold {
    // логика када систем треба да пређе у неактивно стање
    }
}
```,
    caption: [Обрада исхода провере стања активности.]
)<lst:liveness>

Уколико сервис пређе у стање неактивности, систем иницира поновно покретање одговарајућег Docker контејнера. Након успешног поновног покретања, сви бројачи се враћају на почетне вредности, а цео животни циклус надзора почиње поново. На овај начин систем не само да детектује квар, већ и спроводи механизам аутоматског опоравка, обезбеђујући континуитет рада система.

=== Управљање стањем спремности

Стање спремности одражава да ли је сервис у датом тренутку способан да прихвата и обрађује захтеве. За разлику од стања активности, прелазак у стање неспремности не доводи до поновног покретања контејнера, већ служи искључиво за контролу усмеравања саобраћаја у оквиру система.

Сваки неуспех провере спремности повећава бројач узастопних неуспеха, а када он достигне праг отказа, сервис се означава као неспреман. Уколико се провере накнадно стабилизују, сервис се враћа у стање спремности. Ова логика омогућава да сервис остане видљив у систему као функционалан, али привремено недоступан за обраду захтева.

=== Фаза покретања и прелазак у стабилно стање

Стање покретања представља уводну фазу рада сервиса, током које се очекује да сервис изврши све иницијалне радње пре него што буде третиран као у потпуности стабилан. Током ове фазе систем не реагује на неуспехе провера активности и спремности, јер се одређени степен нестабилности сматра уобичајеним приликом подизања сервиса.

Када провере покретања покажу да је сервис стабилан, он прелази у редовно оперативно стање у којем почињу да важе механизми за надзор активности и спремности. Уколико сервис дуже време не успе да превазиђе фазу покретања, односно ако број узастопних неуспеха достигне праг отказа, сервис се означава као нефункционалан без уласка у редовни животни циклус. На овај начин спречава се да сервис остане у стању трајне иницијалне нестабилности.

== Интеракција са _Docker_ окружењем и механизам поновног покретања

Поновно покретање контејнера представља механизам опоравка система у случају када сервис пређе у стање неактивности. Ова функционалност се ослања на резултате провера стања активности и омогућава да систем аутоматски реагује на поремећаје у раду сервиса. Пошто се надгледани сервиси извршавају у _Docker_ контејнерима, неопходна је директна интеграција са _Docker_ окружењем ради контролисаног управљања животним циклусом контејнера.

=== Повезивање са Docker окружењем

Интеграција са _Docker_-ом реализована је у посебном слоју, кроз структуру _Manager_ која садржи _Docker API_ клијент. При иницијализацији система креира се један _Docker_ клијент који користи подешавања окружења (променљиве окружења и верзију _API_-ја), након чега се преко њега приступа _Docker Engine_-у.

На тај начин систем може да пронађе контејнер на основу његовог логичког имена, да приступи основним информацијама о његовом стању и, што је најважније, да спроведе операцију поновног покретања. Сама апликација која ради у контејнеру не мора да буде свесна постојања нашег радног оквира, јер се све управљачке операције извршавају на нивоу _Docker_ окружења.

=== Детекција потребе за поновним покретањем

Потреба за поновним покретањем сервиса настаје онда када провере стања активности дуже време не успевају. Систем за сваки сервис води бројач узастопних неуспеха и, када овај бројач достигне дефинисани праг отказа, сервис се означава као неактиван.

У том тренутку механизам управљања животним циклусом шаље сигнал у унутрашњи канал за пријаву проблема, којим се обавештава главна контролна петља да је потребно спровести поновно покретање контејнера. На тај начин резултати провера стања активности имају директан оперативни ефекат.

=== Поновно покретање контејнера и понашање система након тога

Операција поновног покретања контејнера реализована је у оквиру _Manager_ структуре, која преко _Docker API_-ја позива одговарајућу функцију за поновно покретање. На листингу #link(<lst:docker_restart>)[5] приказана је функција која реализује ову логику.

#figure(
```go
func (m *Manager) RestartContainer(ctx context.Context, name string) error {
    // позив Docker API-ја за поновно покретање контејнера
    return m.cli.ContainerRestart(ctx, name, opts)
}
```,
    caption: [Поновно покретање _Docker_ контејнера преко _Docker API_-ја.]
)<lst:docker_restart>

Функција _RestartContainer_ прима контекст извршавања и име контејнера, припрема опције за заустављање са задатим временским ограничењем и затим позива _Docker API_. На овај начин логика поновног покретања издвојена је у посебан слој, што поједностављује њену употребу у главној петљи и омогућава да се промена начина комуникације са _Docker_ окружењем обави на једном месту.

Након успешног поновног покретања контејнера, систем враћа бројаче неуспеха на почетне вредности и сервис улази у фазу покретања. У тој фази се најпре примењују провере стања покретања, а затим, по стабилизацији сервиса, и провере стања активности и спремности.

== Механизам метрика и алармирања

Поред провера активности и механизма поновног покретања, радни оквир обезбеђује и систем метрика који омогућава праћење понашања сервиса током времена, као и алармирање у случају поремећаја. Метрике пружају увид у динамику рада система и представљају основу за благовремено уочавање неправилности. Алармирање користи управо те податке како би обавестило администраторе када сервис дуже време није у исправном стању.

=== Механизам метрика

Бележење метрика реализује се у складу са _OpenMetrics_ форматом, који је у потпуности усаглашен са _Prometheus_-ом. Систем бележи резултате свих провера активности сервиса и излаже их на путањи _/metrics_ у задатом формату. За сваку проверу записују се:

- исход (успех или неуспех),
- трајање провере,
- број узастопних неуспеха у тренутку мерења,
- назив сервиса и тип провере.

Метрике су конфигурисане као бројачи (_counter_), што je омогућило једноставно праћење трендова током времена. На листингу #link(<lst:metrics>)[6] приказанo је део имплементације који дефинише метрике.

#figure(
```go
// Дефиниција метрике за успешне провере
ProbeSuccess = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: "wh_probe_success_total",
        Help: "Number of successful probe checks.",
    },
    []string{"probe", "target", "container", "probe_type"},
)
```,
    caption: [Дифинисање метрика за успешне провере.]
)<lst:metrics>

#pagebreak()

=== Механизам алармирања

Алармирање у систему реализовано коришћењем стандардног _Prometheus_ екосистема. Систем једино излаже метрике у _OpenMetrics_ формату, док _Prometheus_ преузима те податке и на основу њих покреће правила за аларме дефинисана у _Alertmanager_-у. На тај начин систем остаје фокусирани механизам за надзор и опоравак сервиса, док се логика алармирања препушта специјализованом алату који је за то предвиђен.

Најважнија метрика за алармирање је _wh_container_restarts_total_, која бележи колико пута је сервис поново покренут услед неактивности. На основу ње дефинисан је аларм који се активира ако број поновних покретања прешао задати праг у одређеном временском периоду. На листингу #link(<lst:alert_rule>)[7] приказано је дефинисање правила које детектује учестала поновна покретања контејнера на основу метрике _wh_container_restarts_total_.

#figure(
```yaml
groups:
  - name: whirlpool_launcher_restart_alerts
    interval: 30s
    rules:
      - alert: ContainerRestartBurst
        expr: increase(wh_container_restarts_total[${RESTART_WINDOW_MIN}m]) >= ${RESTART_THRESHOLD}
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.container }} restarted too many times!"
          description: |
            Container: {{ $labels.container }}
            Target endpoint: {{ $labels.target }}
            Probe: {{ $labels.probe }}
            Probe type: {{ $labels.probe_type }}
            Number of restarts: {{ $value }}

```,
    caption: [Дифинисање правила за аларм.]
)<lst:alert_rule>

_Alertmanager_ је конфигурисан да у случају активирања овог правила пошаље _e-mail_ поруку администратору. У глобалној секцији подешени су _SMTP_ параметри, а унутар пријемника _email-alerts_ дефинисан је _HTML_ шаблон за поруку. На листингу #link(<lst:alert_email>)[8] приказана је конфигурација пријемника који шаље _e-mail_ обавештење администратору.

#figure(
```yaml
receivers:
  - name: 'email-alerts'
    email_configs:
      - to: '${ALERT_EMAIL_TO}'
        headers:
          Subject: 'WHIRLPOOL_LAUNCHER ALERT: {{ .GroupLabels.container }}'
        html: |
          <h2>Container restarted too many times!</h2>
          {{ range .Alerts }}
          <h3>{{ .Annotations.summary }}</h3>
          <ul>
            <li><strong>Container:</strong> {{ .Labels.container }}</li>
            <li><strong>Target endpoint:</strong> {{ .Labels.target }}</li>
            <li><strong>Probe:</strong> {{ .Labels.probe }}</li>
            <li><strong>Probe type:</strong> {{ .Labels.probe_type }}</li>
            <li><strong>Restarts:</strong> {{ .Annotations.description }}</li>
          </ul>
          <hr>
          {{ end }}
```,
    caption: [Конфигурација _Alertmanager_ пријемника за _e-mail_ обавештења.]
)<lst:alert_email>

На овај начин је реализована потпуна аутоматизација обавештавања где систем излаже метрике,_Prometheus_ их прикупља, а _Alertmanager_ детектује учестала поновна покретања и шаље упозорење администратору путем _е-mail_-а.

== Тестирање система

Тестирање система изведено је коришћењем посебних тест сервиса који симулирају различита понашања надгледаних сервиса. За сваки подржани протокол _HTTP_, _TCP_ и _gRPC_ припремљене су варијанте сервиса који могу да враћају позитиван одзив или контролисано прелазе у негативно стање након дефинисаног временског периода. На овај начин омогућено је тестирање исправности свих механизама система, као што су провера стања, реакције на деградацију и поновно покретање контејнера у случају учесталих неуспеха.

Систем је тестирањем изложен сценаријима стабилног рада, постепеног погоршавања одзива и потпуне недоступности сервиса. Негативни сценарији омогућили су проверу да ли систем правилно детектује стање неактивности и иницира поновно покретање контејнера након достизања прага за отказ. Тиме је потврђено да систем исправно покрива очекиване случајеве у реалним условима рада.